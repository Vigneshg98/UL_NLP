{"cells":[{"cell_type":"markdown","metadata":{"id":"eMAWKYTn4Nv1"},"source":["# Student Name: VIGNESH GANESAN\n","## Student ID: 23079924"]},{"cell_type":"markdown","source":["**Task 1**"],"metadata":{"id":"vIrrmn3pXOif"}},{"cell_type":"markdown","source":["\n","Class Food\n","\\begin{align*}\n","    & Precision_{Yes} = \\frac{800}{800 + 200} = 0.8 \\\\\n","    & Precision_{No} = \\frac{500}{500 + 200} = 0.714 \\\\\n","    & Recall_{Yes} = \\frac{800}{800 + 200} = 0.8 \\\\\n","    & Recall_{No} = \\frac{500}{500 + 200} = 0.714 \\\\\n","    & F1_{Yes} = 2 \\times \\frac{0.8 \\times 0.8}{0.8 + 0.8} = 0.8\\\\\n","    & F1_{No} = 2 \\times \\frac{0.714 \\times 0.714}{0.714 + 0.714} = 0.714  \n","\\end{align*}\n","\n","\\\\\n","\n","Class Drink\n","\\begin{align*}\n","    & Precision_{Yes} = \\frac{70}{70 + 30} = 0.7 \\\\\n","    & Precision_{No} = \\frac{100}{100 + 30} = 0.769 \\\\\n","    & Recall_{Yes} = \\frac{70}{70 + 30} = 0.7 \\\\\n","    & Recall_{No} = \\frac{100}{100 + 30} = 0.769 \\\\\n","    & F1_{Yes} = 2 \\times \\frac{0.7 \\times 0.7}{0.7 + 0.7} = 0.7\\\\\n","    & F1_{No} = 2 \\times \\frac{0.769 \\times 0.769}{0.769 + 0.769} = 0.769  \n","\\end{align*}\n","\n","\\\\\n","\n","\\begin{align*}\n","    & Microaveraged Precision = \\frac{800+70}{800+200+70+30} = \\frac{870}{1100} = 0.790 \\\\\n","    & Microaveraged Recall = \\frac{800+70}{800+200+70+30} = \\frac{870}{1100} = 0.790 \\\\\n","    & Microaveraged F1 = 2 \\times \\frac{0.790 \\times 0.790}{0.790 + 0.790} = 0.790\n","\\end{align*}\n","\n","\\\\\n","\n","\\begin{align*}\n","    & Macroaveraged Precision = \\frac{0.8+0.7}{2} = 0.746 \\\\\n","    & Macroaveraged Recall = \\frac{0.8+0.7}{2} = 0.746 \\\\\n","    & Macroaveraged F1 = 2 \\times \\frac{0.746 \\times 0.746}{0.746 + 0.746} = 0.746\n","\\end{align*}\n","\n","\\\\\n","\n","Microaveraged Metrics:\n","\n","The microaveraged F1 score is calculated by considering all instances together, regardless of their class. It is the weighted average of the F1 scores, where each instance contributes equally.\n","In this case, the microaveraged F1 score is higher than the individual F1 scores for both \"Food\" and \"Drink\" classes. This is because the \"Food\" class has more instances, and its higher F1 score contributes more to the microaveraged result.\n","\n","\\\\\n","Macroaveraged Metrics:\n","\n","The macroaveraged F1 score is calculated by taking the average of the F1 scores for each class. Each class contributes equally, regardless of the number of instances.\n","The macroaveraged F1 score provides a balanced view across classes. It is slightly lower than the microaveraged F1 score because it considers the lower F1 score of the \"Drink\" class, giving equal importance to both classes.\n","\n","\\\\\n","Class Imbalance Impact:\n","\n","The \"Food\" class, being larger, has a more significant impact on the microaveraged metrics. If the \"Food\" class performs well, it dominates the microaveraged results.\n","The macroaveraged metrics, on the other hand, are more influenced by the performance of the smaller \"Drink\" class. If the \"Drink\" class has a lower F1 score, it brings down the macroaveraged F1 score.\n","\n","\\\\\n","In summary, the difference between microaveraged and macroaveraged F1 scores is influenced by the class imbalance. The microaveraged F1 score is higher because it is dominated by the larger class, while the macroaveraged F1 score provides a more balanced assessment across classes."],"metadata":{"id":"RWdQBwxsc0Qd"}},{"cell_type":"markdown","source":["**Task 2**"],"metadata":{"id":"O8V673uwckHu"}},{"cell_type":"code","source":["import re\n","from collections import Counter"],"metadata":{"id":"09o9fHJKclhB","executionInfo":{"status":"ok","timestamp":1701039570797,"user_tz":0,"elapsed":511,"user":{"displayName":"Vignesh Ganesan","userId":"12355989079067050187"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def normalizeText(text):\n","  return re.findall(r'\\b\\w+\\b', text.lower())\n","\n","def prepareMetadata(trainingSet):\n","  megaDocPLUS = ''\n","  megaDocMINUS = ''\n","  class_word_counts = {}\n","  class_counts = {}\n","  PLUS_bow = {}\n","  MINUS_bow = {}\n","\n","  for doc, label in trainingSet:\n","    words = normalizeText(doc)\n","    if label == '+':\n","      megaDocPLUS += ' '.join(words)\n","      megaDocPLUS += ' '\n","    elif label =='-':\n","      megaDocMINUS += ' '.join(words)\n","      megaDocMINUS += ' '\n","\n","    for word in words:\n","      class_word_counts.setdefault(label, {})\n","      class_word_counts[label][word] = class_word_counts[label].get(word, 0) + 1\n","\n","      class_counts.setdefault(label, 0)\n","      class_counts[label] += 1\n","\n","  megaDocPLUS_split = megaDocPLUS.split()\n","  megaDocMINUS_split = megaDocMINUS.split()\n","  PLUS_bow = [megaDocPLUS_split.count(w) for w in megaDocPLUS_split]\n","  MINUS_bow = [megaDocMINUS_split.count(w) for w in megaDocMINUS_split]\n","  PLUS_bow = dict(zip(megaDocPLUS_split, PLUS_bow))\n","  MINUS_bow = dict(zip(megaDocMINUS_split, MINUS_bow))\n","\n","  V = dict(Counter(PLUS_bow) + Counter(MINUS_bow))\n","\n","  print('megaDocPLUS: ', ' '.join(normalizeText(megaDocPLUS)))\n","  print('megaDocMINUS: ', ' '.join(normalizeText(megaDocMINUS)))\n","  print()\n","\n","  print('class_counts: ', class_counts)\n","  probPLUS = class_counts['+'] / (class_counts['+'] + class_counts['-'])\n","  probMINUS = class_counts['-'] / (class_counts['+'] + class_counts['-'])\n","  print('probPLUS: ', probPLUS)\n","  print('probMINUS: ', probMINUS)\n","  print()\n","\n","  print('PLUS_BOW: ', PLUS_bow)\n","  print('MINUS_BOW: ', MINUS_bow)\n","  print()\n","\n","  print('V: ', V)\n","  print('|V|: ', len(V.keys()))\n","  print()\n","\n","  return probPLUS, probMINUS, PLUS_bow, MINUS_bow, V, class_counts, class_word_counts\n","\n","\n","def naiveBayesClassifier(trainingSet, testSet):\n","    probPLUS, probMINUS, PLUS_bow, MINUS_bow, V, class_counts, class_word_counts = prepareMetadata(trainingSet)\n","    total_docs = len(V)\n","\n","    for test_doc, _ in testSet:\n","      print('Test document: ', test_doc)\n","      docProbPLUS = 0\n","      docProbMINUS = 0\n","      words = normalizeText(test_doc)\n","      wordConditionalProbPLUSSum = 0.0\n","      wordConditionalProbMINUSSum = 0.0\n","      for word in words:\n","        wordConditionalProbPLUS = (PLUS_bow.get(word, 0) + 1) / (class_counts.get('+', 0) + len(V))\n","        wordConditionalProbPLUSSum += wordConditionalProbPLUS\n","        wordConditionalProbMINUS = (MINUS_bow.get(word, 0) + 1) / (class_counts.get('-', 0) + len(V))\n","        wordConditionalProbMINUSSum += wordConditionalProbMINUS\n","        print(f\"Word: '{word}' \\t wordConditionalProbPLUS: {wordConditionalProbPLUS} \\t wordConditionalProbMINUS: {wordConditionalProbMINUS}\")\n","\n","        # word_count = class_word_counts.get('+', {}).get(word, 0) + 1\n","        # class_word_count = class_counts.get('+', 0) + total_docs\n","        # docProbPLUS += (word_count / class_word_count)\n","\n","        # word_count = class_word_counts.get('-', {}).get(word, 0) + 1\n","        # class_word_count = class_counts.get('-', 0) + total_docs\n","        # docProbMINUS += (word_count / class_word_count)\n","      # print(f\"docProbPLUS = {docProbPLUS} \\t docProbMINUS = {docProbMINUS}\")\n","\n","      docProbPLUS = probPLUS * wordConditionalProbPLUSSum\n","      docProbMINUS = probMINUS * wordConditionalProbMINUSSum\n","      print(f\"docProbPLUS = {docProbPLUS} \\t docProbMINUS = {docProbMINUS}\")\n","\n","      inferred_class = \"PLUS\" if docProbPLUS > docProbMINUS else \"MINUS\" if docProbPLUS < docProbMINUS else \"NEUTRAL\"\n","      print('Inferred Class: ', inferred_class)\n","      print('-'*50)\n","\n","trainingSet = [\n","    ('just plain boring', '-'),\n","    ('entirely predictable and lacks energy', '-'),\n","    ('no surprises and very few laughs', '-'),\n","    ('very powerful', '+'),\n","    ('the most fun film of the summer', '+')\n","]\n","testSet = [\n","    ('predictable with no fun', '?')\n","]\n","\n","naiveBayesClassifier(trainingSet, testSet)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3uHFXxc0fFvR","executionInfo":{"status":"ok","timestamp":1701039712858,"user_tz":0,"elapsed":229,"user":{"displayName":"Vignesh Ganesan","userId":"12355989079067050187"}},"outputId":"f0a082b9-d3c8-44e1-97d4-6b827d27d6ac"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["megaDocPLUS:  very powerful the most fun film of the summer\n","megaDocMINUS:  just plain boring entirely predictable and lacks energy no surprises and very few laughs\n","\n","class_counts:  {'-': 14, '+': 9}\n","probPLUS:  0.391304347826087\n","probMINUS:  0.6086956521739131\n","\n","PLUS_BOW:  {'very': 1, 'powerful': 1, 'the': 2, 'most': 1, 'fun': 1, 'film': 1, 'of': 1, 'summer': 1}\n","MINUS_BOW:  {'just': 1, 'plain': 1, 'boring': 1, 'entirely': 1, 'predictable': 1, 'and': 2, 'lacks': 1, 'energy': 1, 'no': 1, 'surprises': 1, 'very': 1, 'few': 1, 'laughs': 1}\n","\n","V:  {'very': 2, 'powerful': 1, 'the': 2, 'most': 1, 'fun': 1, 'film': 1, 'of': 1, 'summer': 1, 'just': 1, 'plain': 1, 'boring': 1, 'entirely': 1, 'predictable': 1, 'and': 2, 'lacks': 1, 'energy': 1, 'no': 1, 'surprises': 1, 'few': 1, 'laughs': 1}\n","|V|:  20\n","\n","Test document:  predictable with no fun\n","Word: 'predictable' \t wordConditionalProbPLUS: 0.034482758620689655 \t wordConditionalProbMINUS: 0.058823529411764705\n","Word: 'with' \t wordConditionalProbPLUS: 0.034482758620689655 \t wordConditionalProbMINUS: 0.029411764705882353\n","Word: 'no' \t wordConditionalProbPLUS: 0.034482758620689655 \t wordConditionalProbMINUS: 0.058823529411764705\n","Word: 'fun' \t wordConditionalProbPLUS: 0.06896551724137931 \t wordConditionalProbMINUS: 0.029411764705882353\n","docProbPLUS = 0.06746626686656672 \t docProbMINUS = 0.10741687979539642\n","Inferred Class:  MINUS\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["Task 3"],"metadata":{"id":"0_CuMsWHghvz"}},{"cell_type":"code","source":["pip install textblob"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lPQ7J5Q7g1lz","executionInfo":{"status":"ok","timestamp":1700832445495,"user_tz":0,"elapsed":8400,"user":{"displayName":"Vignesh Ganesan","userId":"12355989079067050187"}},"outputId":"2f44f9ec-c8ee-4d22-895a-6dad11105ce1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('brown')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeY5idSgg4j4","executionInfo":{"status":"ok","timestamp":1700832457046,"user_tz":0,"elapsed":5375,"user":{"displayName":"Vignesh Ganesan","userId":"12355989079067050187"}},"outputId":"54f35c01-53ce-4df0-ac76-9e29d9b8ff7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","def sentimentAnalyzer(inp_string):\n","    blob = TextBlob(inp_string)\n","    polarity = blob.sentiment.polarity\n","    subjectivity = blob.sentiment.subjectivity\n","    sentiment = \"Positive Sentiment\" if polarity > 0.1 else \"Negative Sentiment\" if polarity < -0.1 else \"Neutral Sentiment\"\n","    confidence = abs(polarity)\n","    return sentiment, confidence, polarity, subjectivity\n","\n","inp_string_list = [\n","    \"NLP is cool\",\n","    \"NLP is cool and useful\",\n","    \"NLP is hard\",\n","    \"NLP is hard and useless\",\n","    \"NLP stands for Natural Language Processing\"\n","]\n","\n","for inp_string in inp_string_list:\n","    sentiment, confidence, polarity, subjectivity = sentimentAnalyzer(inp_string)\n","    print(\"String: \", inp_string)\n","    print(f\"Sentiment(polarity={polarity}, subjectivity={subjectivity})\")\n","    print(sentiment)\n","    print(\"Subjectivity:\", subjectivity)\n","    print(\"\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6yrN-T-CfGOA","executionInfo":{"status":"ok","timestamp":1700848347001,"user_tz":0,"elapsed":2265,"user":{"displayName":"Vignesh Ganesan","userId":"12355989079067050187"}},"outputId":"f67bc295-cf11-4b41-d469-edd52fd17c0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["String:  NLP is cool\n","Sentiment(polarity=0.35, subjectivity=0.65)\n","Positive Sentiment\n","Subjectivity: 0.65\n","\n","String:  NLP is cool and useful\n","Sentiment(polarity=0.32499999999999996, subjectivity=0.325)\n","Positive Sentiment\n","Subjectivity: 0.325\n","\n","String:  NLP is hard\n","Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666)\n","Negative Sentiment\n","Subjectivity: 0.5416666666666666\n","\n","String:  NLP is hard and useless\n","Sentiment(polarity=-0.39583333333333337, subjectivity=0.37083333333333335)\n","Negative Sentiment\n","Subjectivity: 0.37083333333333335\n","\n","String:  NLP stands for Natural Language Processing\n","Sentiment(polarity=0.1, subjectivity=0.4)\n","Neutral Sentiment\n","Subjectivity: 0.4\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YKKqCM75g_Du"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+bDK/zbPAyf528At246vD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}